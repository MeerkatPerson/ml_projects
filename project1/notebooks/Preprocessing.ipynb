{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDSh7zQSAoG7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
        "NANVAL = -998\n",
        "TRAIN = './data/train.csv'\n",
        "TEST = './data/test.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdmLoXd5YmQv"
      },
      "source": [
        "# Nouvelle section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1o7TxTZjz-E"
      },
      "outputs": [],
      "source": [
        "def load_csv_data(data_path, sub_sample=False):\n",
        "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
        "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
        "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
        "    ids = x[:, 0].astype(np.int)\n",
        "    input_data = x[:, 2:]\n",
        "\n",
        "    # convert class labels from strings to binary (-1,1)\n",
        "    yb = np.ones(len(y))\n",
        "    yb[np.where(y=='b')] = -1\n",
        "    \n",
        "    # sub-sample\n",
        "    if sub_sample:\n",
        "        yb = yb[::50]\n",
        "        input_data = input_data[::50]\n",
        "        ids = ids[::50]\n",
        "\n",
        "    return yb, input_data, ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbAcElc1__8E"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-0b2b54bff6ba>:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  ids = x[:, 0].astype(np.int)\n"
          ]
        }
      ],
      "source": [
        "y_train, tx_train, ids_train = load_csv_data(TRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4yFHLVyhHZR",
        "outputId": "511b5728-346c-4272-e6bc-fcbeaf5963f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(250000, 30)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tx_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo7Emr6hlg0R"
      },
      "outputs": [],
      "source": [
        "def standardize(x):\n",
        "    \"\"\"Standardize the original data set.\"\"\"\n",
        "    mean_x = np.mean(x)\n",
        "    x = x - mean_x\n",
        "    std_x = np.std(x)\n",
        "    x = x / std_x\n",
        "    return x, mean_x, std_x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IlZ_OfbAW4z"
      },
      "outputs": [],
      "source": [
        "def preprocessing(y, tx, strategy, std=True, outliers = False):\n",
        "    #TODO : outliers\n",
        "    res_x = tx\n",
        "    res_y = y\n",
        "    res_x = np.where(res_x < NANVAL, np.NaN, res_x)\n",
        "    \n",
        "    \n",
        "    indices = np.where(np.isnan(res_x))\n",
        "    if strategy==0:\n",
        "      # Replace with mean\n",
        "      means = np.nanmean(res_x, axis=0)\n",
        "      res_x[indices] = np.take(means, indices[1]) \n",
        "    elif strategy==1:\n",
        "      # Replace with median\n",
        "      medians = np.nanmedian(res_x, axis=0)\n",
        "      res_x[indices] = np.take(medians, indices[1])\n",
        "    elif strategy==2:\n",
        "      # Remove the NaN\n",
        "      rows_with_nan = ~np.isnan(res_x).any(axis=1)\n",
        "      res_y, res_x = res_y[rows_with_nan], res_x[rows_with_nan]\n",
        "    elif strategy==3:\n",
        "      # Remove columns with NaN\n",
        "      columns_with_nan = ~np.isnan(res_x).any(axis=0)\n",
        "      res_x = res_x[:,columns_with_nan]\n",
        "    elif strategy==4:\n",
        "      # Replace with 0\n",
        "      res_x = np.nan_to_num(res_x)\n",
        "    if outliers : \n",
        "      #TODO remove outliers\n",
        "      pass\n",
        "    if std: \n",
        "      res_x, _, _ = standardize(res_x)\n",
        "    return res_y, res_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.224434, 0.193880, 0.741973, ..., -0.404581, -0.448681,\n",
              "        0.927987],\n",
              "       [1.491133, 0.397023, 0.806170, ..., -0.419301, -0.419301,\n",
              "        0.129434],\n",
              "       [-0.419301, 1.505793, 1.075848, ..., -0.419301, -0.419301,\n",
              "        0.105989],\n",
              "       ...,\n",
              "       [0.832547, 0.299185, 0.480961, ..., -0.419301, -0.419301,\n",
              "        0.079173],\n",
              "       [0.707833, -0.189460, 0.397545, ..., -0.419301, -0.419301,\n",
              "        -0.419301],\n",
              "       [-0.419301, 0.444363, 0.421512, ..., -0.419301, -0.419301,\n",
              "        -0.419301]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y, x = preprocessing(y_train, tx_train, 4) # Replaces by 0\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(250000, 19)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y, x = preprocessing(y_train, tx_train,3) #Removes columns with NaN\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY6VxrFahYHJ",
        "outputId": "909b94c5-6c6e-4513-f502-c0d7e907942c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.565378, -0.081980, 0.262313, ..., -0.457912, -0.485614,\n",
              "        0.379160],\n",
              "       [0.202040, -0.366120, -0.026099, ..., -0.465488, -0.443998,\n",
              "        0.976916],\n",
              "       [0.642063, -0.251942, 0.336545, ..., -0.466182, -0.487791,\n",
              "        0.874140],\n",
              "       ...,\n",
              "       [0.473518, -0.250741, 0.243157, ..., -0.471625, -0.485987,\n",
              "        0.972733],\n",
              "       [0.502778, -0.437943, 0.030811, ..., -0.462849, -0.483675,\n",
              "        3.604720],\n",
              "       [1.151106, -0.115528, 0.001304, ..., -0.467591, -0.475637,\n",
              "        0.831629]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y, x = preprocessing(y_train, tx_train,2) #Removes NaN\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2awx3BWJhWhS",
        "outputId": "0ef9aab5-a95e-4701-d636-3ad2783ffdde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.073985, 0.094295, 0.615337, ..., -0.474629, -0.516552,\n",
              "        0.792170],\n",
              "       [1.327521, 0.287411, 0.676365, ..., -0.488735, -0.488645,\n",
              "        0.033029],\n",
              "       [0.779858, 1.341458, 0.932733, ..., -0.488735, -0.488645,\n",
              "        0.010742],\n",
              "       ...,\n",
              "       [0.701440, 0.194402, 0.367206, ..., -0.488735, -0.488645,\n",
              "        -0.014751],\n",
              "       [0.582882, -0.270126, 0.287908, ..., -0.488735, -0.488645,\n",
              "        -0.488622],\n",
              "       [0.779858, 0.332415, 0.310692, ..., -0.488735, -0.488645,\n",
              "        -0.488622]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y, x = preprocessing(y_train, tx_train, 1) #Replaces with median\n",
        "x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6aMAjEad-Uh",
        "outputId": "6abd8cee-3e65-4828-e73e-6974e91539dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.759992, 0.259607, 0.239990, -0.405605, -0.457325, 3.306876,\n",
              "       -0.490194, -0.461184, -0.405605, 0.366446, -0.472282, -0.496199,\n",
              "       -0.477150, -0.043594, -0.464649, -0.454785, -0.071791, -0.466636,\n",
              "       -0.475329, -0.066767, -0.498084, 0.531177, -0.481820, 0.382568,\n",
              "       -0.481854, -0.481946, 0.105968, -0.481941, -0.481836, -0.481820])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y, x = preprocessing(y_train, tx_train, 0) #Replaces with mean\n",
        "x[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gDYijZ7HlVmB"
      },
      "outputs": [],
      "source": [
        "#implementations.py\n",
        "# /!\\ Most of it is copied from the solutions\n",
        "def least_squares(y, tx):\n",
        "    \"\"\"calculate the least squares.\"\"\"\n",
        "    a = tx.T.dot(tx)\n",
        "    b = tx.T.dot(y)\n",
        "    return np.linalg.solve(a, b)\n",
        "def ridge_regression(y, tx, lamb):\n",
        "    \"\"\"rige regression L2.\"\"\"\n",
        "    aI = lamb * np.identity(tx.shape[1])\n",
        "    a = tx.T.dot(tx) + aI\n",
        "    b = tx.T.dot(y)\n",
        "    return np.linalg.solve(a, b)\n",
        "\n",
        "def sigmoid(x):\n",
        "        return 1./ (1. + np.exp(-x))\n",
        "\n",
        "def regression_loss(y, tx, w):\n",
        "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
        "    pred = sigmoid(tx.dot(w))\n",
        "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
        "    return np.squeeze(- loss)\n",
        "\n",
        "def calculate_gradient(y, tx, w):\n",
        "    \"\"\"compute the gradient of loss.\"\"\"\n",
        "    sigmoids = sigmoid(tx.dot(w)) # N*1\n",
        "    return tx.T.dot((sigmoids - y))\n",
        "\n",
        "def learning_by_gradient_descent(y, tx, w, gamma):\n",
        "    \"\"\"\n",
        "    Do one step of gradient descent using logistic regression.\n",
        "    Return the loss and the updated w.\n",
        "    \"\"\"\n",
        "    #loss = self.regression_loss(y,tx)\n",
        "    grad = calculate_gradient(y, tx, w)\n",
        "    w = w - gamma * grad\n",
        "    return w\n",
        "\n",
        "\n",
        "#Functions we might not use\n",
        "def split_data_equally(x, y, ratio, seed = 1):\n",
        "    \"\"\"\n",
        "      Preserves the distribution\n",
        "    \"\"\"\n",
        "    ind = np.arange(len(y))\n",
        "    classes = set(y)\n",
        "    indices_for_each_class = []\n",
        "    train = np.empty(0, dtype=int)\n",
        "    test = np.empty(0, dtype=int)\n",
        "    for cl in classes :\n",
        "        indices_for_each_class.append(ind[y==cl])\n",
        "    for indices in indices_for_each_class:\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(indices)\n",
        "        cut_ind = int(ratio * len(indices))\n",
        "        train = np.hstack((train, indices[:cut_ind]))\n",
        "        test = np.hstack((test, indices[cut_ind:]))\n",
        "    print(train.shape)\n",
        "    print(test.shape)\n",
        "    return x[train], x[test], y[train], y[test]\n",
        "\n",
        "# probably won't use either\n",
        "def split_data(x, y, ratio, seed=1):\n",
        "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
        "    # set seed\n",
        "    np.random.seed(seed)\n",
        "    # generate random indices\n",
        "    num_row = len(y)\n",
        "    indices = np.random.permutation(num_row)\n",
        "    index_split = int(np.floor(ratio * num_row))\n",
        "    index_tr = indices[: index_split]\n",
        "    index_te = indices[index_split:]\n",
        "    # create split\n",
        "    x_tr = x[index_tr]\n",
        "    x_te = x[index_te]\n",
        "    y_tr = y[index_tr]\n",
        "    y_te = y[index_te]\n",
        "    return x_tr, x_te, y_tr, y_te"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "84f6pepLsi5t"
      },
      "outputs": [],
      "source": [
        "# Classifier.py\n",
        "class Classifier:\n",
        "    \"\"\" \n",
        "    Abstract class to represent a classifier\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\" \n",
        "            Sets the parameters\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Please Implement this method\")\n",
        "        \n",
        "    def train(self, tx_train, y_train):\n",
        "        \"\"\" \n",
        "            Learns a w.\n",
        "            Arguments:\n",
        "                - tx_train: ndarray matrix of size N*D\n",
        "                - y_train: ndarray matrix of size D*1\n",
        "            Hypothesis: tx_train ndd y_train have the same length\n",
        "        \"\"\"        \n",
        "        raise NotImplementedError(\"Please Implement this method\")\n",
        "    \n",
        "    def predict(self, tx):\n",
        "        \"\"\" \n",
        "            Returns a list of predictions. \n",
        "            For linear classifiers with classes {-1,1}, it just returns sign(self.score(x))\n",
        "            Argument:\n",
        "                - tx : N*D dimension\n",
        "            Returns : \n",
        "                List[int] of size N\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Please Implement this method\")\n",
        "\n",
        "    def accuracy(self, predictions, y):\n",
        "        \"\"\"\n",
        "            Computes the accuracy for a list of predictions.\n",
        "            It counts the number of good predictions and divides it by the number of samples.\n",
        "            Arguments :\n",
        "                - predictions : List of size N\n",
        "                - y : List of size N\n",
        "            Returns\n",
        "                float : the accuracy\n",
        "\n",
        "        \"\"\"\n",
        "        print(predictions)\n",
        "        print(y)\n",
        "        return np.sum(predictions==y) / len(y)\n",
        "\n",
        "    def get_params_and_results(self, tx_train, tx_test, y_train, y_test):\n",
        "        \"\"\"\n",
        "            Returns a dictionnary with the parameters and the accuracy\n",
        "            example of output : \n",
        "            {\n",
        "                'name' : 'Classifier',\n",
        "                'accuracy_train' : 0.8, \n",
        "                'accuracy_test' : 0.78,\n",
        "                'params' : \n",
        "                {\n",
        "                    'lambda_' : 0.01,\n",
        "                    'n_iterations' : 10000,\n",
        "                    'gamma' = 0.2\n",
        "                }\n",
        "            \n",
        "            }\n",
        "            Arguments : \n",
        "                tx_train : N * D train set\n",
        "                tx_test : N' * D' test set\n",
        "                y_train : N * 1 train labels \n",
        "                y_test : N' * 1 test labels\n",
        "            Returns :\n",
        "                dictionnary of parameters and accuracy\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Please Implement this method\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cqeDR1-PspNm"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ClassifierRidgeRegression(Classifier):\n",
        "\n",
        "    def __init__(self, lambda_):\n",
        "        \"\"\" \n",
        "            Sets the parameters\n",
        "            Argument:\n",
        "                - lambda_ : float parameter for the ridge regression\n",
        "        \"\"\"\n",
        "        self.lambda_ = lambda_\n",
        "\n",
        "    def train(self, y_train, tx_train):\n",
        "        \"\"\" \n",
        "            Trains the model. It learns a w with ridge regression.\n",
        "            Arguments:\n",
        "                - tx_train: ndarray matrix of size N*D\n",
        "                - y_train: ndarray matrix of size D*1\n",
        "            Hypothesis: tx_train ndd y_train have the same length\n",
        "        \"\"\"\n",
        "        self.w = ridge_regression(y_train, tx_train, self.lambda_)         \n",
        "\n",
        "    def predict(self, tx):\n",
        "        \"\"\" \n",
        "            Returns a list of predictions \n",
        "            Argument:\n",
        "                - tx : N*D dimension\n",
        "            Returns : \n",
        "                List[int] of size N\n",
        "        \"\"\"\n",
        "        return np.sign(tx.dot(self.w))\n",
        "        \n",
        "    def get_params_and_results(self, tx_train, tx_test, y_train, y_test):\n",
        "        \"\"\"\n",
        "            Returns a dictionnary with the parameters and the accuracy\n",
        "            example of output : \n",
        "            {\n",
        "                'name' : 'Classifier',\n",
        "                'accuracy_train' : 0.8, \n",
        "                'accuracy_test' : 0.78,\n",
        "                'params' : \n",
        "                {\n",
        "                    'lambda_' : 0.01,\n",
        "                }\n",
        "            \n",
        "            }\n",
        "            Arguments : \n",
        "                tx_train : N * D train set\n",
        "                tx_test : N' * D' test set\n",
        "                y_train : N * 1 train labels \n",
        "                y_test : N' * 1 test labels\n",
        "            Returns :\n",
        "                dictionnary of parameters and accuracy\n",
        "        \"\"\"\n",
        "        # Compute my predictions\n",
        "        predictions_train = self.predict(tx_train)\n",
        "        predictions_test = self.predict(tx_test)\n",
        "        #Construct a dictionnary of parameters\n",
        "        params = dict()\n",
        "        params['lambda'] = self.lambda_\n",
        "        #construct the final dictionnary\n",
        "        res = dict()\n",
        "        res['name'] = 'ClassifierRidgeRegression' \n",
        "        res['accuracy_train'] = self.accuracy(predictions_train, y_train)\n",
        "        res['accuracy_test'] = self.accuracy(predictions_test, y_test)\n",
        "        res['params'] = params\n",
        "        return res\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yq0pUq-qCrE5"
      },
      "outputs": [],
      "source": [
        "class ClassifierLeastSquares(Classifier):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" \n",
        "            Does not have any parameters to set.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def train(self, y_train, tx_train):\n",
        "        \"\"\" Trains the model. Learns a w with Least Squares. \n",
        "            Arguments:\n",
        "                - tx_train: ndarray matrix of size N*D\n",
        "                - y_train: ndarray matrix of size D*1\n",
        "            Hypothesis: tx_train ndd y_train have the same length\n",
        "        \"\"\"\n",
        "        self.w = least_squares(y_train, tx_train)         \n",
        "        \n",
        "    def predict(self, x):\n",
        "        \"\"\" \n",
        "            Returns a list of predictions.\n",
        "            Argument:\n",
        "                - x: a sample vector 1*D \n",
        "            Returns : \n",
        "                Array[int] \n",
        "        \"\"\"\n",
        "        return np.sign(x.dot(self.w))\n",
        "\n",
        "    def get_params_and_results(self, tx_train, tx_test, y_train, y_test):\n",
        "        \"\"\"\n",
        "            Returns a dictionnary with the parameters and the accuracy\n",
        "            example of output : \n",
        "            {\n",
        "                'name' : 'Classifier',\n",
        "                'accuracy_train' : 0.8, \n",
        "                'accuracy_test' : 0.78,\n",
        "                'params' : {}\n",
        "            }\n",
        "            Arguments : \n",
        "                tx_train : N * D train set\n",
        "                tx_test : N' * D' test set\n",
        "                y_train : N * 1 train labels \n",
        "                y_test : N' * 1 test labels\n",
        "            Returns :\n",
        "                dictionnary of parameters and accuracy\n",
        "        \"\"\"\n",
        "        # Compute my predictions\n",
        "        predictions_train = self.predict(tx_train)\n",
        "        predictions_test = self.predict(tx_test)\n",
        "        #Construct a dictionnary of parameters\n",
        "        params = dict()\n",
        "        #construct the final dictionnary\n",
        "        res = dict()\n",
        "        res['name'] = 'ClassifierLeastSquares' \n",
        "        res['accuracy_train'] = self.accuracy(predictions_train, y_train)\n",
        "        res['accuracy_test'] = self.accuracy(predictions_test, y_test)\n",
        "        res['params'] = params\n",
        "        return res\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YrnZz6-lC-q2"
      },
      "outputs": [],
      "source": [
        "class ClassifierLogisticRegression(Classifier):\n",
        "    \"\"\" \n",
        "    Abstract class to represent a classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma=0.01, n_iterations = 100):\n",
        "        \"\"\" \n",
        "            Sets parameters for logistic regression\n",
        "            Argument:\n",
        "                - gamma (float)\n",
        "                - n_iterations (int)\n",
        "        \"\"\"\n",
        "        self.gamma = gamma\n",
        "        self.n_iterations = n_iterations\n",
        "\n",
        "    def train(self, y_train, tx_train):\n",
        "        \"\"\" \n",
        "            Trains the model. It learns a new w with logistic regression. \n",
        "            Arguments:\n",
        "                - tx_train: ndarray matrix of size N*D\n",
        "                - y_train: ndarray matrix of size D*1\n",
        "            Hypothesis: tx_train ndd y_train have the same length\n",
        "        \"\"\"\n",
        "        self.w = np.empty(tx_train.shape[1])\n",
        "        for _ in range(self.n_iterations):\n",
        "            self.w = learning_by_gradient_descent(y_train, tx_train, self.w, self.gamma)\n",
        "    \n",
        "    \n",
        "    def predict(self, x):\n",
        "        \"\"\" \n",
        "            returns a list of predictions\n",
        "            Argument:\n",
        "                - x: a sample vector 1*D \n",
        "            Returns : \n",
        "                Array[int] \n",
        "        \"\"\"\n",
        "        pred = sigmoid(x.dot(self.w)) \n",
        "        pred = np.asarray([0 if k < 0.5 else 1 for k in pred])\n",
        "        return pred\n",
        "\n",
        "    def get_params_and_results(self, tx_train, tx_test, y_train, y_test):\n",
        "        \"\"\"\n",
        "            Returns a dictionnary with the parameters and the accuracy\n",
        "            example of output : \n",
        "            {\n",
        "                'name' : 'Classifier',\n",
        "                'accuracy_train' : 0.8, \n",
        "                'accuracy_test' : 0.78,\n",
        "                'params' : {}\n",
        "            }\n",
        "            Arguments : \n",
        "                tx_train : N * D train set\n",
        "                tx_test : N' * D' test set\n",
        "                y_train : N * 1 train labels \n",
        "                y_test : N' * 1 test labels\n",
        "            Returns :\n",
        "                dictionnary of parameters and accuracy\n",
        "        \"\"\"\n",
        "        # Compute my predictions\n",
        "        predictions_train = self.predict(tx_train)\n",
        "        predictions_test = self.predict(tx_test)\n",
        "        #Construct a dictionnary of parameters\n",
        "        params = dict()\n",
        "        params['n_iterations'] = self.n_iterations\n",
        "        params['gamma'] = self.gamma\n",
        "        #construct the final dictionnary\n",
        "        res = dict()\n",
        "        res['name'] = 'ClassifierLogisticRegression' \n",
        "        res['accuracy_train'] = self.accuracy(predictions_train, y_train)\n",
        "        res['accuracy_test'] = self.accuracy(predictions_test, y_test)\n",
        "        res['params'] = params\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassifierRandomRidgeRegression(Classifier):\n",
        "\n",
        "    def __init__(self, n_classifier, lambda_, features_per_classifier, degree, use_centroids=True, initial_number_of_features = 30):\n",
        "        self.lambda_= lambda_\n",
        "        self.n_classifier = n_classifier\n",
        "        self.initial_number_of_features = initial_number_of_features\n",
        "        self.features_per_classifier = features_per_classifier\n",
        "        self.degree = degree\n",
        "        self.clf = []\n",
        "        self.features = [] # Each classifier will have random features. We choose them in the train function. Then we need them for our predictions.\n",
        "        self.use_centroids = use_centroids\n",
        "\n",
        "        for i in range(n_classifier):\n",
        "            self.clf.append(ClassifierRidgeRegression(lambda_))\n",
        "\n",
        "    def train(self, y_train, tx_train):\n",
        "        \"\"\" Trains the model. Learns a w with Least Squares. \n",
        "            Arguments:\n",
        "                - tx_train: ndarray matrix of size N*D\n",
        "                - y_train: ndarray matrix of size D*1\n",
        "            Hypothesis: tx_train ndd y_train have the same length\n",
        "        \"\"\"\n",
        "        # CAN ONLY BE USED WITH THE CURRENT VERSION OF BUILD_POLY\n",
        "        \n",
        "        \n",
        "        \n",
        "        #np.random.seed(seed)\n",
        "        \n",
        "        for cl in self.clf:\n",
        "            perm = np.random.permutation(self.initial_number_of_features) # shuffle [0..32]\n",
        "            perm = perm[:self.features_per_classifier] # Takes sqrt first elements\n",
        "            features = [self.degree*k+1+j for k in perm for j in range(self.degree)]\n",
        "            if self.use_centroids:\n",
        "                features.append(tx_train.shape[1]-1)\n",
        "                features.append(tx_train.shape[1]-2)\n",
        "            self.features.append(features)\n",
        "            tx = tx_train[:,features]\n",
        "            cl.train(y_train, tx)        \n",
        "        \n",
        "    def predict(self, x):\n",
        "        \"\"\" \n",
        "            Returns a list of predictions.\n",
        "            Argument:\n",
        "                - x: a sample vector 1*D \n",
        "            Returns : \n",
        "                Array[int] \n",
        "        \"\"\"\n",
        "        preds = np.empty(x.shape[0])\n",
        "\n",
        "        for index, cl in enumerate(self.clf) :\n",
        "            features = self.features[index]\n",
        "            tx = x[:,features]\n",
        "            preds = np.vstack((preds,cl.predict(tx)))\n",
        "        preds = preds.mean(axis = 0)\n",
        "        preds = np.sign(preds)\n",
        "        return preds\n",
        "\n",
        "    def get_params_and_results(self, tx_train, tx_test, y_train, y_test):\n",
        "        \"\"\"\n",
        "            Returns a dictionnary with the parameters and the accuracy\n",
        "            example of output : \n",
        "            {\n",
        "                'name' : 'Classifier',\n",
        "                'accuracy_train' : 0.8, \n",
        "                'accuracy_test' : 0.78,\n",
        "                'params' : {\n",
        "                    'lambda_' : 0.01,\n",
        "                    'n_classifier' : 100,\n",
        "                    'features_per_classifier' : 6,\n",
        "                    'degree' : 7,\n",
        "                    'use_centroids' : True\n",
        "                }\n",
        "            }\n",
        "            Arguments : \n",
        "                tx_train : N * D train set\n",
        "                tx_test : N' * D' test set\n",
        "                y_train : N * 1 train labels \n",
        "                y_test : N' * 1 test labels\n",
        "            Returns :\n",
        "                dictionnary of parameters and accuracy\n",
        "        \"\"\"\n",
        "        # Compute my predictions\n",
        "        predictions_train = self.predict(tx_train)\n",
        "        predictions_test = self.predict(tx_test)\n",
        "        #Construct a dictionnary of parameters\n",
        "        params = dict()\n",
        "        params['lambda_'] = self.lambda_\n",
        "        params['n_classifier'] = self.n_classifier\n",
        "        params['features_per_classifier'] = self.features_per_classifier\n",
        "        params['degree'] = self.degree\n",
        "        params['use_centroids'] = self.use_centroids\n",
        "        #construct the final dictionnary\n",
        "        res = dict()\n",
        "        res['name'] = 'ClassifierRandomRidgeRegression' \n",
        "        res['accuracy_train'] = self.accuracy(predictions_train, y_train)\n",
        "        res['accuracy_test'] = self.accuracy(predictions_test, y_test)\n",
        "        res['params'] = params\n",
        "        print(res)\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Don't uncomment these lines, we don't have a tx_test.\n",
        "# HOW TO USE : \n",
        "# lambda_ = 0.01\n",
        "# clf = ClassifierRidgeRegression(lambda_)\n",
        "# clf.train(y_train, tx_train)\n",
        "# clf.get_params_and_results(tx_train, tx_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4UcpgLjj6pLQ"
      },
      "outputs": [],
      "source": [
        "def build_centroids(y, x):\n",
        "    res = []\n",
        "    for cl in set(y):\n",
        "      res.append(np.mean(x[y==cl], axis = 0))\n",
        "    return res\n",
        "# [centroid class -1, centroid class 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MyWSKiRk5IpA"
      },
      "outputs": [],
      "source": [
        "def kernel(x, centroid):\n",
        "    return np.exp(-np.linalg.norm(x-centroid, axis = 1)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kpHF3vXV2ary"
      },
      "outputs": [],
      "source": [
        "def build_poly(x, degree, functions, centroids):\n",
        "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
        "        also applies functions\n",
        "    \"\"\"\n",
        "    poly = np.ones((len(x), 1))\n",
        "\n",
        "    for i in range(x.shape[1]):\n",
        "        for deg in range(1, degree+1):\n",
        "            poly = np.c_[poly, np.power(x[:, i], deg)]\n",
        "        for f in functions:\n",
        "            poly = np.c_[poly, f(x[:, i])]\n",
        "    for c in centroids:\n",
        "        poly = np.c_[poly, kernel(x,c)]\n",
        "\n",
        "    return poly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dX7TAG294ED",
        "outputId": "a2dbfe99-d7ae-4c71-8645-bbd16b9450a6"
      },
      "outputs": [],
      "source": [
        "#centroides = build_centroids(y,x)\n",
        "#centroides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "#tmp = build_poly(x,7,[], centroides) # This function takes 40 seconds. We can't put it inside a classifier. It would take forever.\n",
        "#tmp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94q6XLKC99k7",
        "outputId": "0badfd21-343f-4477-966a-8743f40916aa"
      },
      "outputs": [],
      "source": [
        "#kernel(x, centroides[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fzKxdvtCyXmO"
      },
      "outputs": [],
      "source": [
        "def build_k_indices(y, k_fold, seed):\n",
        "    \"\"\"build k indices for k-fold.\"\"\"\n",
        "    # TODO : the same function but preserving the distribution\n",
        "    num_row = y.shape[0]\n",
        "    interval = int(num_row / k_fold)\n",
        "    np.random.seed(seed)\n",
        "    indices = np.random.permutation(num_row)\n",
        "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
        "    return np.array(k_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pxWskzv_oDL4"
      },
      "outputs": [],
      "source": [
        "def cross_validation(y, x, k_indices, k, classifier):\n",
        "    \"\"\"return the loss of ridge regression.\"\"\"\n",
        "    # get k'th subgroup in test, others in train\n",
        "    te_indice = k_indices[k]\n",
        "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
        "    tr_indice = tr_indice.reshape(-1)\n",
        "    y_te = y[te_indice]\n",
        "    y_tr = y[tr_indice]\n",
        "    x_te = x[te_indice]\n",
        "    x_tr = x[tr_indice]\n",
        "    # form data with polynomial degree\n",
        "    degree = 4\n",
        "    centroids = build_centroids(y_tr, x_tr)\n",
        "    \n",
        "    tx_tr = build_poly(x_tr, degree, [], centroids) \n",
        "    tx_te = build_poly(x_te, degree, [], centroids) # Important to note : we use the same centroids for the training and the testing\n",
        "    # train\n",
        "    print('train start')\n",
        "    classifier.train(y_tr, tx_tr) \n",
        "    print(\"train over\")\n",
        "    # Return our JSON\n",
        "    return classifier.get_params_and_results(tx_tr, tx_te, y_tr, y_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "J9TEA4p2wzWJ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "def cross_validation_demo(y, x, clf, k_fold = 4, seed=1):\n",
        "    #TODO : add a list of classifiers as an argument\n",
        "    #TODO : add a list of functions for build_poly\n",
        "\n",
        "    # split data in k fold\n",
        "    k_indices = build_k_indices(y, k_fold, seed)\n",
        "    # define lists to store the loss of training data and test data\n",
        "    res = []\n",
        "    # cross validation\n",
        "    for k in range(k_fold):\n",
        "        res.append(cross_validation(y, x, k_indices, k, copy.deepcopy(clf)))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-29-8d3bfeb07fd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0muse_centroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifierRandomRidgeRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_per_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_centroids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_number_of_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_random\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_random\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-28-81d348b400d8>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[1;34m(y, x, clf, k_fold, seed)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# cross validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-27-c161eb44bedc>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, classifier)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mcentroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_centroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtx_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mtx_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Important to note : we use the same centroids for the training and the testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-22-c6db90e6c780>\u001b[0m in \u001b[0;36mbuild_poly\u001b[1;34m(x, degree, functions, centroids)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdeg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mpoly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpoly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mpoly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpoly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "y_random, x_random = preprocessing(y_train, tx_train, 1) \n",
        "\n",
        "\n",
        "n_classifier = 100\n",
        "lambda_ = 0.1\n",
        "initial_number_of_features = 30\n",
        "#features_per_classifier = int(np.sqrt(initial_number_of_features)) + 1\n",
        "features_per_classifier = 15\n",
        "degree = 4 # needs to be the same as we use in cross validation\n",
        "use_centroids = True\n",
        "clf = ClassifierRandomRidgeRegression(n_classifier, lambda_, features_per_classifier, degree, use_centroids, initial_number_of_features)\n",
        "res = cross_validation_demo(y_random, x_random, clf)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5jDpAkO23mY",
        "outputId": "6a94c400-c9db-4267-aac5-adb3425abf2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train start\n",
            "train over\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 1.000000 -1.000000]\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[-1.000000 -1.000000 -1.000000 ... 1.000000 -1.000000 -1.000000]\n",
            "[1.000000 -1.000000 -1.000000 ... 1.000000 -1.000000 -1.000000]\n",
            "train start\n",
            "train over\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 1.000000 -1.000000]\n",
            "[1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "train start\n",
            "train over\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 1.000000 -1.000000]\n",
            "[1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[1.000000 1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 1.000000]\n",
            "train start\n",
            "train over\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 1.000000]\n",
            "[1.000000 1.000000 1.000000 ... -1.000000 1.000000 -1.000000]\n",
            "[1.000000 1.000000 1.000000 ... -1.000000 -1.000000 -1.000000]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'name': 'ClassifierRidgeRegression',\n",
              "  'accuracy_train': 0.7794133333333333,\n",
              "  'accuracy_test': 0.779904,\n",
              "  'params': {'lambda': 0.01}},\n",
              " {'name': 'ClassifierRidgeRegression',\n",
              "  'accuracy_train': 0.7808853333333333,\n",
              "  'accuracy_test': 0.77808,\n",
              "  'params': {'lambda': 0.01}},\n",
              " {'name': 'ClassifierRidgeRegression',\n",
              "  'accuracy_train': 0.7797333333333333,\n",
              "  'accuracy_test': 0.781072,\n",
              "  'params': {'lambda': 0.01}},\n",
              " {'name': 'ClassifierRidgeRegression',\n",
              "  'accuracy_train': 0.779504,\n",
              "  'accuracy_test': 0.779664,\n",
              "  'params': {'lambda': 0.01}}]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ridge Regression\n",
        "lambda_ = 0.01\n",
        "clf = ClassifierRidgeRegression(lambda_) # shouldn't take the input dimension as an argument\n",
        "res = cross_validation_demo(y, x, clf)\n",
        "res\n",
        "# It takes almost 3 minutes to run because we perform build_poly 4 times on the whole dataset.\n",
        "# We could fix it by calling build_poly before splitting the data however there would be a problem with the centroids\n",
        "# as they would be the centroids of the whole dataset instead of the sub_dataset.\n",
        "\n",
        "# Another way to fix this is to split build_poly into 2 functions. poly(tx, degree) which would extend the features (x1^2, x1^3, x2^2, x^3) and centroids(tx) that defines \n",
        "# the centroids for each label(-1 or 1) and calculates the distance between each centroid and each x_n (exp(-||x_n-centroid_i||²)).\n",
        "# We would call poly(tx, degree) inside cross_validation_demo and centroids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train start\n",
            "train over\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 1.000000 -1.000000]\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[-1.000000 -1.000000 -1.000000 ... 1.000000 -1.000000 -1.000000]\n",
            "[1.000000 -1.000000 -1.000000 ... 1.000000 -1.000000 -1.000000]\n",
            "train start\n",
            "train over\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 1.000000 -1.000000]\n",
            "[1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "train start\n",
            "train over\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 1.000000 -1.000000]\n",
            "[1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[1.000000 1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 1.000000]\n",
            "train start\n",
            "train over\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 1.000000]\n",
            "[-1.000000 1.000000 1.000000 ... -1.000000 1.000000 -1.000000]\n",
            "[1.000000 1.000000 1.000000 ... -1.000000 -1.000000 -1.000000]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'name': 'ClassifierLeastSquares',\n",
              "  'accuracy_train': 0.7857973333333333,\n",
              "  'accuracy_test': 0.786736,\n",
              "  'params': {}},\n",
              " {'name': 'ClassifierLeastSquares',\n",
              "  'accuracy_train': 0.7869866666666666,\n",
              "  'accuracy_test': 0.784464,\n",
              "  'params': {}},\n",
              " {'name': 'ClassifierLeastSquares',\n",
              "  'accuracy_train': 0.7866506666666667,\n",
              "  'accuracy_test': 0.785952,\n",
              "  'params': {}},\n",
              " {'name': 'ClassifierLeastSquares',\n",
              "  'accuracy_train': 0.7855466666666666,\n",
              "  'accuracy_test': 0.787616,\n",
              "  'params': {}}]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lambda_ = 0.01\n",
        "clf = ClassifierLeastSquares() # shouldn't take the input dimension as an argument\n",
        "res = cross_validation_demo(y, x, clf)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train start\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-7878ab50f770>:16: RuntimeWarning: overflow encountered in exp\n",
            "  return 1./ (1. + np.exp(-x))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train over\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[1.000000 0.000000 0.000000 ... 1.000000 0.000000 0.000000]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-31-1cbaa59a1836>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifierLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mformated_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Logistic Regression doesn't work with labels {-1, 1} but only {0, 1}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformated_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mres\u001b[0m \u001b[1;31m# We get better results when we don't use feature expansion.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-27-1e4a5aad86b2>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[1;34m(y, x, clf, k_fold, seed)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# cross validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-26-bb334fb8194f>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, classifier)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mtx_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtx_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Important to note : we use the same centroids for the training and the testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train start'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-21-c6db90e6c780>\u001b[0m in \u001b[0;36mbuild_poly\u001b[1;34m(x, degree, functions, centroids)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdeg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mpoly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpoly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mpoly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpoly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "gamma = 0.1\n",
        "n_iterations = 10 \n",
        "clf = ClassifierLogisticRegression(gamma, n_iterations)\n",
        "formated_y = np.where(y<1, 0, y) # Logistic Regression doesn't work with labels {-1, 1} but only {0, 1}\n",
        "res = cross_validation_demo(formated_y, x, clf)\n",
        "res # We get better results when we don't use feature expansion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, we get bad results. I think it is because of the build_poly function.\n",
        "Have a look at the results with a build_poly of degree 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_validation_logistic(y, x, k_indices, k, classifier, degree):\n",
        "    \"\"\"return the loss of ridge regression.\"\"\"\n",
        "    # get k'th subgroup in test, others in train\n",
        "    te_indice = k_indices[k]\n",
        "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
        "    tr_indice = tr_indice.reshape(-1)\n",
        "    y_te = y[te_indice]\n",
        "    y_tr = y[tr_indice]\n",
        "    x_te = x[te_indice]\n",
        "    x_tr = x[tr_indice]\n",
        "    # form data with polynomial degree\n",
        "    centroids = build_centroids(y_tr, x_tr)\n",
        "    \n",
        "    tx_tr = build_poly(x_tr, degree, [], centroids) \n",
        "    tx_te = build_poly(x_te, degree, [], centroids) # Important to note : we use the same centroids for the training and the testing\n",
        "    # train\n",
        "    print('train start')\n",
        "    classifier.train(y_tr, tx_tr) \n",
        "    print(\"train over\")\n",
        "    # Return our JSON\n",
        "    return classifier.get_params_and_results(tx_tr, tx_te, y_tr, y_te)\n",
        "def cross_validation_demo_logistic(y, x, clf, degree, k_fold = 4, seed=1):\n",
        "    #TODO : add a list of classifiers as an argument\n",
        "    #TODO : add a list of functions for build_poly\n",
        "\n",
        "    # split data in k fold\n",
        "    k_indices = build_k_indices(y, k_fold, seed)\n",
        "    # define lists to store the loss of training data and test data\n",
        "    res = []\n",
        "    # cross validation\n",
        "    for k in range(k_fold):\n",
        "        res.append(cross_validation_logistic(y, x, k_indices, k, copy.deepcopy(clf), degree))\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train start\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-7878ab50f770>:16: RuntimeWarning: overflow encountered in exp\n",
            "  return 1./ (1. + np.exp(-x))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train over\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0.000000 1.000000 0.000000 ... 1.000000 0.000000 0.000000]\n",
            "[0 0 0 ... 0 0 1]\n",
            "[0.000000 1.000000 0.000000 ... 1.000000 0.000000 1.000000]\n",
            "train start\n",
            "train over\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0.000000 1.000000 0.000000 ... 1.000000 0.000000 0.000000]\n",
            "[0 0 0 ... 0 0 1]\n",
            "[0.000000 1.000000 0.000000 ... 0.000000 0.000000 1.000000]\n",
            "train start\n",
            "train over\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0.000000 1.000000 0.000000 ... 1.000000 0.000000 0.000000]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0.000000 0.000000 1.000000 ... 0.000000 0.000000 1.000000]\n",
            "train start\n",
            "train over\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0.000000 1.000000 0.000000 ... 0.000000 0.000000 1.000000]\n",
            "[1 1 0 ... 0 0 0]\n",
            "[1.000000 1.000000 1.000000 ... 1.000000 0.000000 0.000000]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'name': 'ClassifierLogisticRegression',\n",
              "  'accuracy_train': 0.606882781301386,\n",
              "  'accuracy_test': 0.6102302090674183,\n",
              "  'params': {'n_iterations': 100, 'gamma': 0.1}},\n",
              " {'name': 'ClassifierLogisticRegression',\n",
              "  'accuracy_train': 0.6149087776994754,\n",
              "  'accuracy_test': 0.6129903688043223,\n",
              "  'params': {'n_iterations': 100, 'gamma': 0.1}},\n",
              " {'name': 'ClassifierLogisticRegression',\n",
              "  'accuracy_train': 0.6175710594315246,\n",
              "  'accuracy_test': 0.6191566831101715,\n",
              "  'params': {'n_iterations': 100, 'gamma': 0.1}},\n",
              " {'name': 'ClassifierLogisticRegression',\n",
              "  'accuracy_train': 0.6206444287839636,\n",
              "  'accuracy_test': 0.61892177589852,\n",
              "  'params': {'n_iterations': 100, 'gamma': 0.1}}]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y2, x2 = preprocessing(y_train, tx_train, 2, std=False) # Fill the NaN with 0, maybe we should try without standardization\n",
        "y2 = np.where(y2<1, 0, y2) # Logistic Regression doesn't work with labels {-1, 1} but only {0, 1}\n",
        "\n",
        "gamma = 0.1\n",
        "n_iterations = 100\n",
        "degree = 1\n",
        "clf = ClassifierLogisticRegression(gamma, n_iterations)\n",
        "res = cross_validation_demo_logistic(y2, x2, clf, degree)\n",
        "res # We get better results when we don't use feature expansion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_poly_interaction(x, degree, functions, centroids):\n",
        "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
        "        also applies functions\n",
        "    \"\"\"\n",
        "    poly = np.ones((len(x), 1))\n",
        "    for i in range(x.shape[1]):\n",
        "        for deg in range(1, degree+1):\n",
        "            poly = np.c_[poly, np.power(x[:, i], deg)]\n",
        "        for f in functions:\n",
        "            poly = np.c_[poly, f(x[:, i])]\n",
        "        for j in range(i+1, x.shape[1]):\n",
        "            poly = np.c_[poly, x[:,i] * x[:,j]]\n",
        "            \n",
        "    for c in centroids:\n",
        "        poly = np.c_[poly, kernel(x,c)]\n",
        "\n",
        "    return poly\n",
        "\n",
        "def cross_validation_interaction(y, x, k_indices, k, classifier):\n",
        "    \"\"\"return the loss of ridge regression.\"\"\"\n",
        "    # get k'th subgroup in test, others in train\n",
        "    te_indice = k_indices[k]\n",
        "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
        "    tr_indice = tr_indice.reshape(-1)\n",
        "    y_te = y[te_indice]\n",
        "    y_tr = y[tr_indice]\n",
        "    x_te = x[te_indice]\n",
        "    x_tr = x[tr_indice]\n",
        "    # form data with polynomial degree\n",
        "    degree = 7\n",
        "    centroids = build_centroids(y_tr, x_tr)\n",
        "    \n",
        "    tx_tr = build_poly_interaction(x_tr, degree, [], centroids) \n",
        "    tx_te = build_poly_interaction(x_te, degree, [], centroids) # Important to note : we use the same centroids for the training and the testing\n",
        "    # train\n",
        "    print('train start')\n",
        "    classifier.train(y_tr, tx_tr) \n",
        "    print(\"train over\")\n",
        "    # Return our JSON\n",
        "    return classifier.get_params_and_results(tx_tr, tx_te, y_tr, y_te)\n",
        "\n",
        "def cross_validation_demo_interaction(y, x, clf, k_fold = 4, seed=1):\n",
        "    #TODO : add a list of classifiers as an argument\n",
        "    #TODO : add a list of functions for build_poly\n",
        "\n",
        "    # split data in k fold\n",
        "    k_indices = build_k_indices(y, k_fold, seed)\n",
        "    # define lists to store the loss of training data and test data\n",
        "    res = []\n",
        "    # cross validation\n",
        "    for k in range(k_fold):\n",
        "        tmp = cross_validation_interaction(y, x, k_indices, k, copy.deepcopy(clf))\n",
        "        print(tmp)\n",
        "        res.append(tmp)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n",
            "290\n",
            "300\n",
            "310\n",
            "320\n",
            "330\n",
            "340\n",
            "350\n",
            "360\n",
            "370\n",
            "380\n",
            "390\n",
            "400\n",
            "410\n",
            "420\n",
            "430\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n",
            "290\n",
            "300\n",
            "310\n",
            "320\n",
            "330\n",
            "340\n",
            "350\n",
            "360\n",
            "370\n",
            "380\n",
            "390\n",
            "400\n",
            "410\n",
            "420\n",
            "430\n",
            "train start\n",
            "train over\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 1.000000 -1.000000]\n",
            "[-1.000000 -1.000000 -1.000000 ... -1.000000 -1.000000 -1.000000]\n",
            "[-1.000000 -1.000000 -1.000000 ... 1.000000 -1.000000 -1.000000]\n",
            "[1.000000 -1.000000 -1.000000 ... 1.000000 -1.000000 -1.000000]\n",
            "{'name': 'ClassifierRidgeRegression', 'accuracy_train': 0.8142933333333333, 'accuracy_test': 0.81176, 'params': {'lambda': 0.01}}\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n",
            "290\n",
            "300\n",
            "310\n",
            "320\n",
            "330\n",
            "340\n",
            "350\n",
            "360\n",
            "370\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-42-eddc563f3e8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifierRidgeRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# shouldn't take the input dimension as an argument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_demo_interaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-41-5f4961b3bba7>\u001b[0m in \u001b[0;36mcross_validation_demo_interaction\u001b[1;34m(y, x, clf, k_fold, seed)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# cross validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation_interaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-41-5f4961b3bba7>\u001b[0m in \u001b[0;36mcross_validation_interaction\u001b[1;34m(y, x, k_indices, k, classifier)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mcentroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_centroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mtx_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly_interaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mtx_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly_interaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Important to note : we use the same centroids for the training and the testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-41-5f4961b3bba7>\u001b[0m in \u001b[0;36mbuild_poly_interaction\u001b[1;34m(x, degree, functions, centroids)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mk\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpoly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpoly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lambda_ = 0.01\n",
        "clf = ClassifierRidgeRegression(lambda_) # shouldn't take the input dimension as an argument\n",
        "res = cross_validation_demo_interaction(y, x, clf)\n",
        "res"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Preprocessing.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
