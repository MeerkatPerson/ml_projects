{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils for ClassifierLogisticRegression\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Compute the sigmoid function on t.\"\"\"\n",
    "    return 1./ (1+ np.exp(-t))\n",
    "\n",
    "\n",
    "def der_sigmoid(t):\n",
    "    \"\"\"Compute derivtive of sigmoid on t\"\"\"\n",
    "    return sigmoid(t)*(1-sigmoid(t))\n",
    "\n",
    "\n",
    "def log_likelihood_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w)) \n",
    "    return ((y - pred)**2).mean()\n",
    "\n",
    "\n",
    "\n",
    "def calculate_gradient(y, x, w):\n",
    "    \"\"\"compute the gradient of log_likelihood_loss wrt weights.\"\"\"\n",
    "    temp = y.ravel()\n",
    "    arg = x@w\n",
    "    s = sigmoid(arg)\n",
    "    gradient = x.T@(s - temp)\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma, return_gradient = False):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    w = w.ravel()\n",
    "    #compute loss\n",
    "    loss = log_likelihood_loss(y, tx, w)\n",
    "    #copmute gradient\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    #do gradient step \n",
    "    w -= gamma*grad\n",
    "\n",
    "    #if required, return also the gradient\n",
    "    if return_gradient:\n",
    "        return loss, w, grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "class ClassifierLogisticRegression():\n",
    "\n",
    "    def update_params(self):\n",
    "        \"\"\"Update the dictionary containng the parameters\"\"\"\n",
    "        self.params['name'] = self.name\n",
    "        self.params['lambda_'] = self.lambda_\n",
    "        self.params['regulairizer'] = self.regularizer\n",
    "        self.params['gamma'] = self.gamma\n",
    "        self.params['max_iterations'] = self.max_iterations\n",
    "        self.params['threshold'] = self.threshold\n",
    "    \n",
    "    def __init__(self, lambda_, regularizer, gamma, max_iterations , threshold):\n",
    "        \"\"\" \n",
    "            Sets parameters for logistic regression\n",
    "            Argument:\n",
    "                - gamma (float)\n",
    "                - n_iterations (int)\n",
    "        \"\"\"\n",
    "\n",
    "        #A dictionary containing the relevant parameters of the classifier \n",
    "        self.params = dict()\n",
    "\n",
    "        #name of the classifier \n",
    "        self.name = 'LogisticRegression'\n",
    "        \n",
    "        #weight of the regularization term in the loss\n",
    "        self.lambda_= lambda_\n",
    "        \n",
    "        #kind of regularizer: L1, L2 or None\n",
    "        self.regularizer = regularizer\n",
    "\n",
    "        #the step in gradient descent\n",
    "        self.gamma = gamma\n",
    "\n",
    "        #the maximum number of iterations\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "        #threshold in gradient descent\n",
    "        self.threshold = threshold\n",
    "        \n",
    "\n",
    "        self.update_params()\n",
    "\n",
    "    def train(self, y_train, tx_train, batch_size = -1, verbose = True, tx_validation = None, y_validation = None, store_gradient = False):\n",
    "        \"\"\" \n",
    "            Trains the model. It learns a new w with logistic regression. \n",
    "            Arguments:\n",
    "                - tx_train: ndarray matrix of size N*D\n",
    "                - y_train: ndarray matrix of size D*1\n",
    "            Hypothesis: tx_train ndd y_train have the same length\n",
    "        \"\"\"\n",
    "        #initialize the weights\n",
    "        self.w = np.zeros(tx_train.shape[1])\n",
    "        #store the losses over 1 complete iteration (epoch)\n",
    "        self.losses = []\n",
    "        #store the prediction accuracies if validation tests are inputted:\n",
    "        if (tx_validation is not None) and (y_validation is not None):\n",
    "            self.pred_accuracies_train = []\n",
    "            self.pred_accuracies_validation = []\n",
    "        #store the norm of the gradient if required\n",
    "        if store_gradient:\n",
    "            self.stored_gradients = []\n",
    "\n",
    "        #initiazlie the number of samples\n",
    "        N = tx_train.shape[0]\n",
    "        #initialize the batch size\n",
    "        if batch_size == -1:\n",
    "            batch_size = N\n",
    "\n",
    "        #iterate over the dataset\n",
    "        for iter in range(self.max_iterations):\n",
    "            \n",
    "            #loss accumulated over many batches\n",
    "            acc_loss = 0\n",
    "            for b in range(0, N, batch_size):  \n",
    "                \n",
    "                #perform a gradient step over a batch\n",
    "                #if required, get also the gradient\n",
    "                if store_gradient:\n",
    "                    l, self.w, grad = learning_by_gradient_descent(\n",
    "                        y_train[b:b+batch_size], \n",
    "                        tx_train[b:b+batch_size], \n",
    "                        self.w, \n",
    "                        self.gamma, \n",
    "                        return_gradient = True)\n",
    "                    \n",
    "                else:\n",
    "                    l, self.w = learning_by_gradient_descent(\n",
    "                        y_train[b:b+batch_size], \n",
    "                        tx_train[b:b+batch_size], \n",
    "                        self.w, \n",
    "                        self.gamma\n",
    "                        )\n",
    "            \n",
    "                #update accumulated loss\n",
    "                acc_loss += l\n",
    "\n",
    "            #output the loss if verbose\n",
    "            if verbose and iter % 100 == 0:\n",
    "                print(\"Current iteration={a}, loss={b}\".format(a=iter, b=acc_loss))\n",
    "            \n",
    "            #if required, store the predictions log\n",
    "            if (tx_validation is not None) and (y_validation is not None):\n",
    "                self.pred_accuracies_train += [(self.predict(tx_train) == y_train).mean()]\n",
    "                self.pred_accuracies_validation += [(self.predict(tx_validation) == y_validation).mean()]\n",
    "\n",
    "            #if required, store the norm of the gradient\n",
    "            if store_gradient:\n",
    "                self.stored_gradients += [np.linalg.norm(grad)]\n",
    "\n",
    "            #store the loss over an iteration\n",
    "            self.losses += [acc_loss]\n",
    "            \n",
    "\n",
    "            #check if convergence has been achieved\n",
    "            if len(self.losses) > 1 and np.abs(self.losses[-1] - self.losses[-2]) < self.threshold:\n",
    "            \n",
    "                #update internal parameters and exit\n",
    "                self.params['losses'] = self.losses\n",
    "                self.params['weights'] = self.w\n",
    "                print('hit thresh')\n",
    "\n",
    "                #if accuracies were required:\n",
    "                if (tx_validation is not None) and (y_validation is not None):\n",
    "                    self.params['accuyracues_while_training_train'] = self.pred_accuracies_train\n",
    "                    self.params['accuyracues_while_training_validation'] = self.pred_accuracies_validation                    \n",
    "\n",
    "                #if required, store the norm of the gradient\n",
    "                if store_gradient:\n",
    "                    self.params['stored_gradients'] = self.stored_gradients\n",
    "                \n",
    "                break\n",
    "\n",
    "        #end of training: update internal parameters and exit\n",
    "        self.params['losses'] = self.losses\n",
    "        self.params['weights'] = self.w\n",
    "\n",
    "        #if accuracies were required:\n",
    "        if (tx_validation is not None) and (y_validation is not None):\n",
    "            self.params['accuyracues_while_training_train'] = self.pred_accuracies_train\n",
    "            self.params['accuyracues_while_training_validation'] = self.pred_accuracies_validation\n",
    "    \n",
    "        #if required, store the norm of the gradient\n",
    "        if store_gradient:\n",
    "            self.params['stored_gradients'] = self.stored_gradients\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\" \n",
    "            returns a list of predictions\n",
    "            Argument:\n",
    "                - x: a sample vector 1*D \n",
    "            Returns : \n",
    "                Array[int] \n",
    "        \"\"\"\n",
    "        pred = sigmoid(x.dot(self.w)) \n",
    "        pred = np.rint(pred)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils for logistic regressin\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from helpers import sample_data, load_data, standardize\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts') \n",
    "\n",
    "\n",
    "from proj1_helpers import *         # Not necessary to copy function for loading CSV data anymore\n",
    "from preprocessing_helpers import * # All function related to preprocessing are now in this helper script \n",
    "                                    # (in the scripts-directory) \n",
    "\n",
    "\n",
    "#from least_squares import least_squares\n",
    "from plots import visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data.\n",
    "height, weight, gender = load_data()\n",
    "\n",
    "seed = 1\n",
    "# build sampled x and y.\n",
    "y = np.expand_dims(gender, axis=1)\n",
    "X = np.c_[height.reshape(-1), weight.reshape(-1)]\n",
    "y, X = sample_data(y, X, seed, size_samples=200)\n",
    "x, mean_x, std_x = standardize(X)\n",
    "tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "\n",
    "y = 1.0*y\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ClassifierLogisticRegression(\n",
    "    lambda_ = 0, \n",
    "    regularizer = None, \n",
    "    gamma=0.01, \n",
    "    max_iterations = 1000, \n",
    "    threshold = 1e-8\n",
    "    )\n",
    "\n",
    "clf.train(y_train=y, tx_train=tx, batch_size = -1, tx_validation = tx, y_validation = y)\n",
    "plt.plot(clf.losses)\n",
    "fig = plt.figure()\n",
    "visualization(y, x, mean_x, std_x, clf.w.T, \"classification_by_logistic_regression_gradient_descent\", True)\n",
    "(clf.predict(tx) == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainig_plots(clf):\n",
    "    fig, axis = plt.subplots(1, 2, figsize = (20, 5))\n",
    "    axis[0].plot(clf.params['losses'], label = 'Loss')\n",
    "    axis[0].set_xlabel('Iteration')\n",
    "    axis[0].set_ylabel('MSE Loss')\n",
    "    axis[0].set_title('Loss')\n",
    "    axis[0].legend()\n",
    "\n",
    "    axis[1].plot(clf.params['accuyracues_while_training_train'], label = 'accuracy training')\n",
    "    axis[1].plot(clf.params['accuyracues_while_training_validation'], label = 'accuracy validation')\n",
    "    axis[1].set_xlabel('Iteration')\n",
    "    axis[1].set_ylabel('Fraction of success')\n",
    "    axis[0].set_title('Overfit test')\n",
    "    axis[1].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ClassifierLogisticRegression(\n",
    "    lambda_ = 0, \n",
    "    regularizer = None, \n",
    "    gamma=0.01, \n",
    "    max_iterations = 200, \n",
    "    threshold = 1e-8\n",
    "    )\n",
    "\n",
    "clf.train(y_train=y, tx_train=tx, batch_size = 20)\n",
    "plt.plot(clf.losses)\n",
    "fig = plt.figure()\n",
    "visualization(y, x, mean_x, std_x, clf.w.T, \"classification_by_logistic_regression_gradient_descent\", True)\n",
    "(clf.predict(tx) == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y=='b')] = -1\n",
    "    \n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "def preprocessing(y, tx, nan_strategy, standardize_=True, outliers = False):\n",
    "  \"\"\"\n",
    "  Do the preprocessing of the data.\n",
    "  Argument:\n",
    "      - y : of shape (N, )\n",
    "      - tx: of shape (N, D)\n",
    "      - nan_strategy: string. Defines how we handle the data. One of the following:\n",
    "          1. 'NanToMean', replaces NaNs with the mean\n",
    "          2. 'NanToMedian', replaces the NaNs with the median\n",
    "          3. 'RemoveNan', removes the rows containing the NaNs\n",
    "          4. 'RemoveNanFeatures' removes the columns with NaNs\n",
    "          5. 'NanTo0', replaces the NaNs with 0\n",
    "      - standardize: standardizes the data\n",
    "      - outliers TODO\n",
    "\n",
    "  Returns : \n",
    "      -(res_y, res_x) : tuple of processed data\n",
    "\n",
    "  \"\"\"\n",
    "  NANVAL = -998\n",
    "  \n",
    "  #TODO : outliers\n",
    "  res_x = tx\n",
    "  res_y = y\n",
    "  res_x = np.where(res_x < NANVAL, np.NaN, res_x)\n",
    "\n",
    "  indices = np.where(np.isnan(res_x))\n",
    "  if nan_strategy=='NanToMean':\n",
    "    # Replace with mean\n",
    "    means = np.nanmean(res_x, axis=0)\n",
    "    res_x[indices] = np.take(means, indices[1]) \n",
    "  elif nan_strategy=='NanToMedian':\n",
    "    # Replace with median\n",
    "    medians = np.nanmedian(res_x, axis=0)\n",
    "    res_x[indices] = np.take(medians, indices[1])\n",
    "  elif nan_strategy=='RemoveNan':\n",
    "    # Remove the NaN\n",
    "    rows_with_nan = ~np.isnan(res_x).any(axis=1)\n",
    "    res_y, res_x = res_y[rows_with_nan], res_x[rows_with_nan]\n",
    "  elif nan_strategy=='RemoveNanFeatures':\n",
    "    # Remove columns with NaN\n",
    "    columns_with_nan = ~np.isnan(res_x).any(axis=0)\n",
    "    res_x = res_x[:,columns_with_nan]\n",
    "  elif nan_strategy == 'OnlyNanFeatures':\n",
    "    columns_wo_nan = np.isnan(res_x).any(axis=0)\n",
    "    res_x = res_x[:,columns_wo_nan]\n",
    "    rows_with_nan = ~np.isnan(res_x).any(axis=1)\n",
    "    res_y, res_x = res_y[rows_with_nan], res_x[rows_with_nan]\n",
    "  elif nan_strategy== 'NanTo0':\n",
    "    # Replace with 0\n",
    "    res_x = np.nan_to_num(res_x)\n",
    "  else:\n",
    "    raise Error('specify a correct strategy')\n",
    "\n",
    "\n",
    "  if outliers : \n",
    "    #TODO remove outliers\n",
    "    pass\n",
    "  if standardize_: \n",
    "    res_x, _, _ = standardize(res_x)\n",
    "  return res_y, res_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, verbose = False):\n",
    "    n_samples = y_train.shape[0]\n",
    "\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    sub_x_1 = x[indices][:int(ratio*n_samples)]\n",
    "    sub_x_2 = x[indices][int(ratio*n_samples):]\n",
    "\n",
    "    sub_y_1 = y[indices][:int(ratio*n_samples)]\n",
    "    sub_y_2 = y[indices][int(ratio*n_samples):]\n",
    "\n",
    "    if verbose:\n",
    "        print('ration:\\t', ratio)\n",
    "        print('ratio of samples 1st subset:\\t', np.round_((sub_y_1 == 1).sum()/(ratio*n_samples), decimals=2))\n",
    "        print('ratio of samples 2nd subset:\\t', np.round_((sub_y_2 == 1).sum()/((1-ratio)*n_samples), decimals=2))\n",
    "    return sub_x_1, sub_y_1, sub_x_2, sub_y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/v5cl87nj5q5_f0wrtyh4g4kw0000gn/T/ipykernel_20699/2786025188.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ids = x[:, 0].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "#real stuff now\n",
    "\n",
    "TRAIN = '../data/train.csv' # due to directory structure, the data directory is now one directory above this one\n",
    "TEST = '../data/test.csv'\n",
    "y, x, ids_train = load_csv_data('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NanToMean\n",
      "Sanity check :n =  250000 D =  30\n",
      "Current iteration=0, loss=400.30889162930157\n"
     ]
    }
   ],
   "source": [
    "clf = ClassifierLogisticRegression(\n",
    "    lambda_ = 0, \n",
    "    regularizer = None, \n",
    "    gamma= 1e-5, \n",
    "    max_iterations = 100, \n",
    "    threshold = 1e-8\n",
    "    )\n",
    "\n",
    "strategies = [\n",
    "    'NanToMean']        #replaces NaNs with the mean\n",
    "\n",
    "dictionaries = []\n",
    "for strat in strategies:\n",
    "    print(strat)\n",
    "    #clf = ClassifierLinearRegression(lambda_ = 0.2, regularizer = reg)\n",
    "    y_train, tx_train = preprocessing(y, x, strat, standardize_=True) \n",
    "    y_train = (y_train+1)/2\n",
    "    print('Sanity check :n = ', tx_train.shape[0], 'D = ', tx_train.shape[1])\n",
    "    tx_train, y_train, tx_validation, y_validation = split_data(tx_train, y_train, ratio = 0.7, verbose = False)\n",
    "    clf.train(y_train, tx_train, verbose = True, batch_size = 100, tx_validation = tx_validation, y_validation = y_validation, store_gradient=True)\n",
    "    #dictionary = clf.get_params_and_results(tx_train, tx_validation, y_train, y_validation)\n",
    "    #dictionaries += [dictionary]\n",
    "    #print('accuracy_test', np.round(dictionary['accuracy_test'], decimals = 2))\n",
    "    #print('accuracy_train', np.round(dictionary['accuracy_train'], decimals = 2))\n",
    "    #print('regularization', clf.regularizer, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the store gradient methods stores the gradient on the last batch iteration if batch == True\n",
    "len(clf.params['stored_gradients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainig_plots(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clf.params['stored_gradients'])\n",
    "plt.ylabel('norm of the gradients (log-scale)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Norm of gradients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clf.predict(tx_train) == y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clf.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate relationship between batch_size, number of dimensions, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".... as multiprocessing didn't seem to work in Jupyter notebook (I had this issue before, it seems to be a thing), I am working on this in a 'normal' Python script under project1/scripts/logReg_gridSearch.py."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f89a89aa964b5e0e29a0e1d0886160f39ebbf2130b1a0b091fb1771632a4492"
  },
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
