{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331f7831",
   "metadata": {},
   "source": [
    "# 📥 Making predictions / classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e779cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts') \n",
    "\n",
    "from proj1_helpers import *         # Not necessary to copy function for loading CSV data anymore\n",
    "from preprocessing_helpers import * # All function related to preprocessing are now in this helper script \n",
    "                                    # (in the scripts-directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d634a4",
   "metadata": {},
   "source": [
    "## (1) Read in the pre-processed data written by Preprocessing.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c7bb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../data/train.csv' # due to directory structure, the data directory is now one directory above this one\n",
    "TEST = '../data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7183031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, tx_train, ids_train = load_csv_data(TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee0523",
   "metadata": {},
   "source": [
    "## (2) Functions required by the different types of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9c17c",
   "metadata": {},
   "source": [
    "### Model a) Least Squares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb4cede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd3f6a",
   "metadata": {},
   "source": [
    "### Model b) Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ce188f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    sigmoids = sigmoid(tx.dot(w)) # N*1\n",
    "    return tx.T.dot((sigmoids - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daa86608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    #loss = self.regression_loss(y,tx)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c549e",
   "metadata": {},
   "source": [
    "### Model c) Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f94eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"rige regression L2.\"\"\"\n",
    "    aI = lamb * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2b6ad",
   "metadata": {},
   "source": [
    "### Model d) Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48cd7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "        return 1./ (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56056467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return np.squeeze(- loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ded7fe",
   "metadata": {},
   "source": [
    "## (3) Classifier classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c76a3",
   "metadata": {},
   "source": [
    "### Classifier a) Abstract base class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c4bbc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier.py\n",
    "class Classifier:\n",
    "    \"\"\" \n",
    "    Abstract class to represent a classifier\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "            Sets the parameters\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "        \n",
    "    def train(self, tx_train, y_train):\n",
    "        \"\"\" \n",
    "            Learns a w.\n",
    "            Arguments:\n",
    "                - tx_train: ndarray matrix of size N*D\n",
    "                - y_train: ndarray matrix of size D*1\n",
    "            Hypothesis: tx_train ndd y_train have the same length\n",
    "        \"\"\"        \n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "    \n",
    "    def predict(self, tx):\n",
    "        \"\"\" \n",
    "            Returns a list of predictions. \n",
    "            For linear classifiers with classes {-1,1}, it just returns sign(self.score(x))\n",
    "            Argument:\n",
    "                - tx : N*D dimension\n",
    "            Returns : \n",
    "                List[int] of size N\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def accuracy(self, predictions, y):\n",
    "        \"\"\"\n",
    "            Computes the accuracy for a list of predictions.\n",
    "            It counts the number of good predictions and divides it by the number of samples.\n",
    "            Arguments :\n",
    "                - predictions : List of size N\n",
    "                - y : List of size N\n",
    "            Returns\n",
    "                float : the accuracy\n",
    "\n",
    "        \"\"\"\n",
    "        print(predictions)\n",
    "        print(y)\n",
    "        return np.sum(predictions==y) / len(y)\n",
    "\n",
    "    def get_params_and_results(self, tx_train, tx_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "            Returns a dictionnary with the parameters and the accuracy\n",
    "            example of output : \n",
    "            {\n",
    "                'name' : 'Classifier',\n",
    "                'accuracy_train' : 0.8, \n",
    "                'accuracy_test' : 0.78,\n",
    "                'params' : \n",
    "                {\n",
    "                    'lambda_' : 0.01,\n",
    "                    'n_iterations' : 10000,\n",
    "                    'gamma' = 0.2\n",
    "                }\n",
    "            \n",
    "            }\n",
    "            Arguments : \n",
    "                tx_train : N * D train set\n",
    "                tx_test : N' * D' test set\n",
    "                y_train : N * 1 train labels \n",
    "                y_test : N' * 1 test labels\n",
    "            Returns :\n",
    "                dictionnary of parameters and accuracy\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c4ac0",
   "metadata": {},
   "source": [
    "### Classifier b) Ridge Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c9e0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't uncomment these lines, we don't have a tx_test.\n",
    "# HOW TO USE : \n",
    "# lambda_ = 0.01\n",
    "# clf = ClassifierRidgeRegression(lambda_)\n",
    "# clf.train(y_train, tx_train)\n",
    "# clf.get_params_and_results(tx_train, tx_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6f67c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierRidgeRegression(Classifier):\n",
    "\n",
    "    def __init__(self, lambda_):\n",
    "        \"\"\" \n",
    "            Sets the parameters\n",
    "            Argument:\n",
    "                - lambda_ : float parameter for the ridge regression\n",
    "        \"\"\"\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def train(self, y_train, tx_train):\n",
    "        \"\"\" \n",
    "            Trains the model. It learns a w with ridge regression.\n",
    "            Arguments:\n",
    "                - tx_train: ndarray matrix of size N*D\n",
    "                - y_train: ndarray matrix of size D*1\n",
    "            Hypothesis: tx_train ndd y_train have the same length\n",
    "        \"\"\"\n",
    "        self.w = ridge_regression(y_train, tx_train, self.lambda_)         \n",
    "\n",
    "    def predict(self, tx):\n",
    "        \"\"\" \n",
    "            Returns a list of predictions \n",
    "            Argument:\n",
    "                - tx : N*D dimension\n",
    "            Returns : \n",
    "                List[int] of size N\n",
    "        \"\"\"\n",
    "        return np.sign(tx.dot(self.w))\n",
    "        \n",
    "    def get_params_and_results(self, tx_train, tx_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "            Returns a dictionnary with the parameters and the accuracy\n",
    "            example of output : \n",
    "            {\n",
    "                'name' : 'Classifier',\n",
    "                'accuracy_train' : 0.8, \n",
    "                'accuracy_test' : 0.78,\n",
    "                'params' : \n",
    "                {\n",
    "                    'lambda_' : 0.01,\n",
    "                }\n",
    "            \n",
    "            }\n",
    "            Arguments : \n",
    "                tx_train : N * D train set\n",
    "                tx_test : N' * D' test set\n",
    "                y_train : N * 1 train labels \n",
    "                y_test : N' * 1 test labels\n",
    "            Returns :\n",
    "                dictionnary of parameters and accuracy\n",
    "        \"\"\"\n",
    "        # Compute my predictions\n",
    "        predictions_train = self.predict(tx_train)\n",
    "        predictions_test = self.predict(tx_test)\n",
    "        #Construct a dictionnary of parameters\n",
    "        params = dict()\n",
    "        params['lambda'] = self.lambda_\n",
    "        #construct the final dictionnary\n",
    "        res = dict()\n",
    "        res['name'] = 'ClassifierRidgeRegression' \n",
    "        res['accuracy_train'] = self.accuracy(predictions_train, y_train)\n",
    "        res['accuracy_test'] = self.accuracy(predictions_test, y_test)\n",
    "        res['params'] = params\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f49c429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierRandomRidgeRegression(Classifier):\n",
    "\n",
    "    def __init__(self, n_classifier, lambda_, features_per_classifier, degree, use_centroids=True, initial_number_of_features = 30):\n",
    "        self.lambda_= lambda_\n",
    "        self.n_classifier = n_classifier\n",
    "        self.initial_number_of_features = initial_number_of_features\n",
    "        self.features_per_classifier = features_per_classifier\n",
    "        self.degree = degree\n",
    "        self.clf = []\n",
    "        self.features = [] # Each classifier will have random features. We choose them in the train function. Then we need them for our predictions.\n",
    "        self.use_centroids = use_centroids\n",
    "\n",
    "        for i in range(n_classifier):\n",
    "            self.clf.append(ClassifierRidgeRegression(lambda_))\n",
    "\n",
    "    def train(self, y_train, tx_train):\n",
    "        \"\"\" Trains the model. Learns a w with Least Squares. \n",
    "            Arguments:\n",
    "                - tx_train: ndarray matrix of size N*D\n",
    "                - y_train: ndarray matrix of size D*1\n",
    "            Hypothesis: tx_train ndd y_train have the same length\n",
    "        \"\"\"\n",
    "        # CAN ONLY BE USED WITH THE CURRENT VERSION OF BUILD_POLY\n",
    "        \n",
    "        #np.random.seed(seed)\n",
    "        \n",
    "        for cl in self.clf:\n",
    "            perm = np.random.permutation(self.initial_number_of_features) # shuffle [0..32]\n",
    "            perm = perm[:self.features_per_classifier] # Takes sqrt first elements\n",
    "            features = [self.degree*k+1+j for k in perm for j in range(self.degree)]\n",
    "            if self.use_centroids:\n",
    "                features.append(tx_train.shape[1]-1)\n",
    "                features.append(tx_train.shape[1]-2)\n",
    "            self.features.append(features)\n",
    "            tx = tx_train[:,features]\n",
    "            cl.train(y_train, tx)        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\" \n",
    "            Returns a list of predictions.\n",
    "            Argument:\n",
    "                - x: a sample vector 1*D \n",
    "            Returns : \n",
    "                Array[int] \n",
    "        \"\"\"\n",
    "        preds = np.empty(x.shape[0])\n",
    "\n",
    "        for index, cl in enumerate(self.clf) :\n",
    "            features = self.features[index]\n",
    "            tx = x[:,features]\n",
    "            preds = np.vstack((preds,cl.predict(tx)))\n",
    "        preds = preds.mean(axis = 0)\n",
    "        preds = np.sign(preds)\n",
    "        return preds\n",
    "\n",
    "    def get_params_and_results(self, tx_train, tx_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "            Returns a dictionnary with the parameters and the accuracy\n",
    "            example of output : \n",
    "            {\n",
    "                'name' : 'Classifier',\n",
    "                'accuracy_train' : 0.8, \n",
    "                'accuracy_test' : 0.78,\n",
    "                'params' : {\n",
    "                    'lambda_' : 0.01,\n",
    "                    'n_classifier' : 100,\n",
    "                    'features_per_classifier' : 6,\n",
    "                    'degree' : 7,\n",
    "                    'use_centroids' : True\n",
    "                }\n",
    "            }\n",
    "            Arguments : \n",
    "                tx_train : N * D train set\n",
    "                tx_test : N' * D' test set\n",
    "                y_train : N * 1 train labels \n",
    "                y_test : N' * 1 test labels\n",
    "            Returns :\n",
    "                dictionnary of parameters and accuracy\n",
    "        \"\"\"\n",
    "        # Compute my predictions\n",
    "        predictions_train = self.predict(tx_train)\n",
    "        predictions_test = self.predict(tx_test)\n",
    "        #Construct a dictionnary of parameters\n",
    "        params = dict()\n",
    "        params['lambda_'] = self.lambda_\n",
    "        params['n_classifier'] = self.n_classifier\n",
    "        params['features_per_classifier'] = self.features_per_classifier\n",
    "        params['degree'] = self.degree\n",
    "        params['use_centroids'] = self.use_centroids\n",
    "        #construct the final dictionnary\n",
    "        res = dict()\n",
    "        res['name'] = 'ClassifierRandomRidgeRegression' \n",
    "        res['accuracy_train'] = self.accuracy(predictions_train, y_train)\n",
    "        res['accuracy_test'] = self.accuracy(predictions_test, y_test)\n",
    "        res['params'] = params\n",
    "        print(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b83425",
   "metadata": {},
   "source": [
    "### Classifier c) Least Squares Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "647f5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierLeastSquares(Classifier):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "            Does not have any parameters to set.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train(self, y_train, tx_train):\n",
    "        \"\"\" Trains the model. Learns a w with Least Squares. \n",
    "            Arguments:\n",
    "                - tx_train: ndarray matrix of size N*D\n",
    "                - y_train: ndarray matrix of size D*1\n",
    "            Hypothesis: tx_train ndd y_train have the same length\n",
    "        \"\"\"\n",
    "        self.w = least_squares(y_train, tx_train)         \n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\" \n",
    "            Returns a list of predictions.\n",
    "            Argument:\n",
    "                - x: a sample vector 1*D \n",
    "            Returns : \n",
    "                Array[int] \n",
    "        \"\"\"\n",
    "        return np.sign(x.dot(self.w))\n",
    "\n",
    "    def get_params_and_results(self, tx_train, tx_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "            Returns a dictionnary with the parameters and the accuracy\n",
    "            example of output : \n",
    "            {\n",
    "                'name' : 'Classifier',\n",
    "                'accuracy_train' : 0.8, \n",
    "                'accuracy_test' : 0.78,\n",
    "                'params' : {}\n",
    "            }\n",
    "            Arguments : \n",
    "                tx_train : N * D train set\n",
    "                tx_test : N' * D' test set\n",
    "                y_train : N * 1 train labels \n",
    "                y_test : N' * 1 test labels\n",
    "            Returns :\n",
    "                dictionnary of parameters and accuracy\n",
    "        \"\"\"\n",
    "        # Compute my predictions\n",
    "        predictions_train = self.predict(tx_train)\n",
    "        predictions_test = self.predict(tx_test)\n",
    "        #Construct a dictionnary of parameters\n",
    "        params = dict()\n",
    "        #construct the final dictionnary\n",
    "        res = dict()\n",
    "        res['name'] = 'ClassifierLeastSquares' \n",
    "        res['accuracy_train'] = self.accuracy(predictions_train, y_train)\n",
    "        res['accuracy_test'] = self.accuracy(predictions_test, y_test)\n",
    "        res['params'] = params\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aae65d",
   "metadata": {},
   "source": [
    "### Classifier d) Log Reg Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60f48ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierLogisticRegression(Classifier):\n",
    "    \"\"\" \n",
    "    Abstract class to represent a classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.01, n_iterations = 100):\n",
    "        \"\"\" \n",
    "            Sets parameters for logistic regression\n",
    "            Argument:\n",
    "                - gamma (float)\n",
    "                - n_iterations (int)\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "    def train(self, y_train, tx_train):\n",
    "        \"\"\" \n",
    "            Trains the model. It learns a new w with logistic regression. \n",
    "            Arguments:\n",
    "                - tx_train: ndarray matrix of size N*D\n",
    "                - y_train: ndarray matrix of size D*1\n",
    "            Hypothesis: tx_train ndd y_train have the same length\n",
    "        \"\"\"\n",
    "        self.w = np.empty(tx_train.shape[1])\n",
    "        for _ in range(self.n_iterations):\n",
    "            self.w = learning_by_gradient_descent(y_train, tx_train, self.w, self.gamma)\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\" \n",
    "            returns a list of predictions\n",
    "            Argument:\n",
    "                - x: a sample vector 1*D \n",
    "            Returns : \n",
    "                Array[int] \n",
    "        \"\"\"\n",
    "        pred = sigmoid(x.dot(self.w)) \n",
    "        pred = np.asarray([0 if k < 0.5 else 1 for k in pred])\n",
    "        return pred\n",
    "\n",
    "    def get_params_and_results(self, tx_train, tx_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "            Returns a dictionnary with the parameters and the accuracy\n",
    "            example of output : \n",
    "            {\n",
    "                'name' : 'Classifier',\n",
    "                'accuracy_train' : 0.8, \n",
    "                'accuracy_test' : 0.78,\n",
    "                'params' : {}\n",
    "            }\n",
    "            Arguments : \n",
    "                tx_train : N * D train set\n",
    "                tx_test : N' * D' test set\n",
    "                y_train : N * 1 train labels \n",
    "                y_test : N' * 1 test labels\n",
    "            Returns :\n",
    "                dictionnary of parameters and accuracy\n",
    "        \"\"\"\n",
    "        # Compute my predictions\n",
    "        predictions_train = self.predict(tx_train)\n",
    "        predictions_test = self.predict(tx_test)\n",
    "        #Construct a dictionnary of parameters\n",
    "        params = dict()\n",
    "        params['n_iterations'] = self.n_iterations\n",
    "        params['gamma'] = self.gamma\n",
    "        #construct the final dictionnary\n",
    "        res = dict()\n",
    "        res['name'] = 'ClassifierLogisticRegression' \n",
    "        res['accuracy_train'] = self.accuracy(predictions_train, y_train)\n",
    "        res['accuracy_test'] = self.accuracy(predictions_test, y_test)\n",
    "        res['params'] = params\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf7951",
   "metadata": {},
   "source": [
    "## (4) Split the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93046377",
   "metadata": {},
   "source": [
    "### Strategy a) Splitting according to a ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70f1e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions we might not use\n",
    "def split_data_equally(x, y, ratio, seed = 1):\n",
    "    \"\"\"\n",
    "      Preserves the distribution\n",
    "    \"\"\"\n",
    "    ind = np.arange(len(y))\n",
    "    classes = set(y)\n",
    "    indices_for_each_class = []\n",
    "    train = np.empty(0, dtype=int)\n",
    "    test = np.empty(0, dtype=int)\n",
    "    for cl in classes :\n",
    "        indices_for_each_class.append(ind[y==cl])\n",
    "    for indices in indices_for_each_class:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "        cut_ind = int(ratio * len(indices))\n",
    "        train = np.hstack((train, indices[:cut_ind]))\n",
    "        test = np.hstack((test, indices[cut_ind:]))\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    return x[train], x[test], y[train], y[test]\n",
    "\n",
    "# probably won't use either\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ed559",
   "metadata": {},
   "source": [
    "### Strategy b) Splitting using k-indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91cf2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    # TODO : the same function but preserving the distribution\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b7916e",
   "metadata": {},
   "source": [
    "## (5) Cross-Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "941ee789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, classifier):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    # form data with polynomial degree\n",
    "    degree = 4\n",
    "    centroids = build_centroids(y_tr, x_tr)\n",
    "    \n",
    "    tx_tr = build_poly(x_tr, degree, [], centroids) \n",
    "    tx_te = build_poly(x_te, degree, [], centroids) # Important to note : we use the same centroids for the training and the testing\n",
    "    # train\n",
    "    print('train start')\n",
    "    classifier.train(y_tr, tx_tr) \n",
    "    print(\"train over\")\n",
    "    # Return our JSON\n",
    "    return classifier.get_params_and_results(tx_tr, tx_te, y_tr, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acae59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def cross_validation_demo(y, x, clf, k_fold = 4, seed=1):\n",
    "    #TODO : add a list of classifiers as an argument\n",
    "    #TODO : add a list of functions for build_poly\n",
    "\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    res = []\n",
    "    # cross validation\n",
    "    for k in range(k_fold):\n",
    "        res.append(cross_validation(y, x, k_indices, k, copy.deepcopy(clf)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4103ee",
   "metadata": {},
   "source": [
    "### Below cross-validation demos by Ali 🐸 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "561cba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start\n",
      "train over\n",
      "[-1. -1. -1. ... -1.  1. -1.]\n",
      "[-1. -1. -1. ... -1. -1. -1.]\n",
      "[-1. -1. -1. ...  1. -1. -1.]\n",
      "[ 1. -1. -1. ...  1. -1. -1.]\n",
      "{'name': 'ClassifierRandomRidgeRegression', 'accuracy_train': 0.7744906666666667, 'accuracy_test': 0.774752, 'params': {'lambda_': 0.1, 'n_classifier': 100, 'features_per_classifier': 15, 'degree': 4, 'use_centroids': True}}\n",
      "train start\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b0/v5cl87nj5q5_f0wrtyh4g4kw0000gn/T/ipykernel_6465/164319504.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0muse_centroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifierRandomRidgeRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_per_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_centroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_number_of_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/b0/v5cl87nj5q5_f0wrtyh4g4kw0000gn/T/ipykernel_6465/3469724433.py\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[0;34m(y, x, clf, k_fold, seed)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/b0/v5cl87nj5q5_f0wrtyh4g4kw0000gn/T/ipykernel_6465/2488060109.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, classifier)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train over\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Return our JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/b0/v5cl87nj5q5_f0wrtyh4g4kw0000gn/T/ipykernel_6465/2200731726.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, y_train, tx_train)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_random, x_random = preprocessing(y_train, tx_train, 1) \n",
    "\n",
    "\n",
    "n_classifier = 100\n",
    "lambda_ = 0.1\n",
    "initial_number_of_features = 30\n",
    "#features_per_classifier = int(np.sqrt(initial_number_of_features)) + 1\n",
    "features_per_classifier = 15\n",
    "degree = 4 # needs to be the same as we use in cross validation\n",
    "use_centroids = True\n",
    "clf = ClassifierRandomRidgeRegression(n_classifier, lambda_, features_per_classifier, degree, use_centroids, initial_number_of_features)\n",
    "res = cross_validation_demo(y_random, x_random, clf)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b12809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "lambda_ = 0.01\n",
    "clf = ClassifierRidgeRegression(lambda_) # shouldn't take the input dimension as an argument\n",
    "res = cross_validation_demo(y, x, clf)\n",
    "res\n",
    "# It takes almost 3 minutes to run because we perform build_poly 4 times on the whole dataset.\n",
    "# We could fix it by calling build_poly before splitting the data however there would be a problem with the centroids\n",
    "# as they would be the centroids of the whole dataset instead of the sub_dataset.\n",
    "\n",
    "# Another way to fix this is to split build_poly into 2 functions. poly(tx, degree) which would extend the features (x1^2, x1^3, x2^2, x^3) and centroids(tx) that defines \n",
    "# the centroids for each label(-1 or 1) and calculates the distance between each centroid and each x_n (exp(-||x_n-centroid_i||²)).\n",
    "# We would call poly(tx, degree) inside cross_validation_demo and centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.01\n",
    "clf = ClassifierLeastSquares() # shouldn't take the input dimension as an argument\n",
    "res = cross_validation_demo(y, x, clf)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "n_iterations = 10 \n",
    "clf = ClassifierLogisticRegression(gamma, n_iterations)\n",
    "formated_y = np.where(y<1, 0, y) # Logistic Regression doesn't work with labels {-1, 1} but only {0, 1}\n",
    "res = cross_validation_demo(formated_y, x, clf)\n",
    "res # We get better results when we don't use feature expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f1e9d",
   "metadata": {},
   "source": [
    "As you can see, we get bad results. I think it is because of the build_poly function.\n",
    "Have a look at the results with a build_poly of degree 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fd79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_logistic(y, x, k_indices, k, classifier, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    # form data with polynomial degree\n",
    "    centroids = build_centroids(y_tr, x_tr)\n",
    "    \n",
    "    tx_tr = build_poly(x_tr, degree, [], centroids) \n",
    "    tx_te = build_poly(x_te, degree, [], centroids) # Important to note : we use the same centroids for the training and the testing\n",
    "    # train\n",
    "    print('train start')\n",
    "    classifier.train(y_tr, tx_tr) \n",
    "    print(\"train over\")\n",
    "    # Return our JSON\n",
    "    return classifier.get_params_and_results(tx_tr, tx_te, y_tr, y_te)\n",
    "def cross_validation_demo_logistic(y, x, clf, degree, k_fold = 4, seed=1):\n",
    "    #TODO : add a list of classifiers as an argument\n",
    "    #TODO : add a list of functions for build_poly\n",
    "\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    res = []\n",
    "    # cross validation\n",
    "    for k in range(k_fold):\n",
    "        res.append(cross_validation_logistic(y, x, k_indices, k, copy.deepcopy(clf), degree))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2, x2 = preprocessing(y_train, tx_train, 2, std=False) # Fill the NaN with 0, maybe we should try without standardization\n",
    "y2 = np.where(y2<1, 0, y2) # Logistic Regression doesn't work with labels {-1, 1} but only {0, 1}\n",
    "\n",
    "gamma = 0.1\n",
    "n_iterations = 100\n",
    "degree = 1\n",
    "clf = ClassifierLogisticRegression(gamma, n_iterations)\n",
    "res = cross_validation_demo_logistic(y2, x2, clf, degree)\n",
    "res # We get better results when we don't use feature expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aee03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_demo_interaction(y, x, clf, k_fold = 4, seed=1):\n",
    "    #TODO : add a list of classifiers as an argument\n",
    "    #TODO : add a list of functions for build_poly\n",
    "\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    res = []\n",
    "    # cross validation\n",
    "    for k in range(k_fold):\n",
    "        tmp = cross_validation_interaction(y, x, k_indices, k, copy.deepcopy(clf))\n",
    "        print(tmp)\n",
    "        res.append(tmp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.01\n",
    "clf = ClassifierRidgeRegression(lambda_) # shouldn't take the input dimension as an argument\n",
    "res = cross_validation_demo_interaction(y, x, clf)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_interaction(y, x, k_indices, k, classifier):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    # form data with polynomial degree\n",
    "    degree = 7\n",
    "    centroids = build_centroids(y_tr, x_tr)\n",
    "    \n",
    "    tx_tr = build_poly_interaction(x_tr, degree, [], centroids) \n",
    "    tx_te = build_poly_interaction(x_te, degree, [], centroids) # Important to note : we use the same centroids for the training and the testing\n",
    "    # train\n",
    "    print('train start')\n",
    "    classifier.train(y_tr, tx_tr) \n",
    "    print(\"train over\")\n",
    "    # Return our JSON\n",
    "    return classifier.get_params_and_results(tx_tr, tx_te, y_tr, y_te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
